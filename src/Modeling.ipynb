{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/dad/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tempfile import TemporaryFile\n",
    "from PIL import Image\n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "# ---------------------------\n",
    "# ------ MODELING -----------\n",
    "from keras.preprocessing.image import array_to_img, \\\n",
    "    img_to_array, load_img\n",
    "    \n",
    "    \n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of rows:  98128\n",
      "2nd row of file indexing \n",
      " ['beech_v02.jpg', '../images/beech_v02.jpg', 560, 'beech']\n",
      "white_oak\n",
      "maple\n",
      "red_oak\n",
      "walnut\n",
      "beech\n",
      "cherry\n"
     ]
    }
   ],
   "source": [
    "# didn't save dictionary of wood names\n",
    "# rather than passing it \n",
    "# I'll recreate here\n",
    "def get_file_list(location = '../images/*.*'):\n",
    "    return sorted(glob.glob(location))\n",
    "# files in /images folder\n",
    "filelist_images = get_file_list()\n",
    "\n",
    "# save filelist_images to done.csv\n",
    "\n",
    "done_files = np.array(filelist_images)\n",
    "np.savetxt('../images/done/done.csv', \n",
    "           done_files, \n",
    "           delimiter=\",\",\n",
    "           fmt='%s')\n",
    "\n",
    "# open done.csv file and save as done list\n",
    "completed = pd.read_csv('../images/done/done.csv', header = None)\n",
    "is_done = np.array(completed)\n",
    "done = []\n",
    "length = len(is_done)\n",
    "for i in range(length):\n",
    "    done.append(is_done[i][0])\n",
    "    \n",
    "# Verifies that no new files to add\n",
    "filelist_images == done\n",
    "# ------------ FIXED FOR ALL FILES -------------\n",
    "size = 180 # size of image\n",
    "border = 5 # remove any weirdness at egde\n",
    "\n",
    "# Create loop through each file\n",
    "total_rows = 0\n",
    "file_indexing = [[]]\n",
    "images_dict = {}\n",
    "for file in filelist_images:\n",
    "    image = Image.open(file)\n",
    "    array = np.array(image)\n",
    "    \n",
    "    # array in now grey scale\n",
    "    height = array.shape[0]\n",
    "    width = array.shape[1]\n",
    "    steps_h = int((height-2*border)/(size/2))\n",
    "    steps_w = int((width-2*border)/(size/2))\n",
    "    num_slices = (steps_w-1) * (steps_h-1)\n",
    "    num_rows = num_slices * 8\n",
    "    file.split('ges/')[1]\n",
    "    stuff = [file.split('ges/')[1], file, num_rows, file.split('ges/')[1].split('_v')[0]]\n",
    "    file_indexing.append(stuff)\n",
    "    total_rows += num_rows\n",
    "   \n",
    "if [] in file_indexing:\n",
    "    file_indexing.remove([])\n",
    "    \n",
    "print('total number of rows: ', total_rows)\n",
    "print('2nd row of file indexing \\n', file_indexing[1])\n",
    "\n",
    "# get number of catagories and \n",
    "# list of woods being looked at\n",
    "names = []\n",
    "for row in range(len(file_indexing)):\n",
    "    names.append(file_indexing[row][3])\n",
    "unique_names = set(names)\n",
    "unique_names = list(unique_names)\n",
    "wood_names_map = {}\n",
    "\n",
    "for i in range(len(unique_names)):\n",
    "    print(unique_names[i])\n",
    "    wood_names_map[unique_names[i]] = i\n",
    "\n",
    "# print(wood_names_map)\n",
    "wood_index_map = {v: k for k, v in wood_names_map.items()}\n",
    "# print(wood_index_map)\n",
    "# for line in file_indexing:\n",
    "#     print(file_indexing[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open Files\n",
    "X = np.load('../data/X.npy')\n",
    "y = np.load('../data/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "tts = train_test_split\n",
    "X_train, X_test, y_train, y_test = tts(\n",
    "    X, y, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "cm = confusion_matrix\n",
    "cr = classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train, and y_train (78502, 180, 180) (78502, 1)\n",
      "X_test, and y_test (19626, 180, 180) (19626, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check to see what they look like\n",
    "print(\"X_train, and y_train\", X_train.shape, y_train.shape)\n",
    "print(\"X_test, and y_test\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train, and Y_train (78502, 180, 180) (78502, 6)\n",
      "X_test, and Y_test (19626, 180, 180) (19626, 6)\n"
     ]
    }
   ],
   "source": [
    "nb_classes = len(unique_names)\n",
    "# convert class vectors to binary class matrices (don't change)\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes) # cool\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)   # cool * 2\n",
    "# They should look different now\n",
    "print(\"X_train, and Y_train\", X_train.shape, Y_train.shape)\n",
    "print(\"X_test, and Y_test\", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['white_oak', 'maple', 'red_oak', 'walnut', 'beech', 'cherry']\n",
      "180\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50 # number of training samples used at a time to update the weights\n",
    "nb_epoch = 4    # number of passes\n",
    "# input image dimensions\n",
    "img_rows, img_cols = size, size \n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 8\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2) # decreases image size, and helps to avoid overfitting\n",
    "# convolution kernel size\n",
    "kernel_size = (4, 4) # slides over image to learn features\n",
    "# reshape image for Keras, note that image_dim_ordering set in ~.keras/keras.json\n",
    "# don't change any of this\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (78502, 180, 180, 1)\n",
      "78502 train samples\n",
      "19626 test samples\n"
     ]
    }
   ],
   "source": [
    "# don't change conversion or normalization\n",
    "X_train = X_train.astype('float32') # data was uint8 [0-255]\n",
    "X_test = X_test.astype('float32')  # data was uint8 [0-255]\n",
    "X_train /= 255 # normalizing (scaling from 0 to 1)\n",
    "X_test /= 255  # normalizing (scaling from 0 to 1)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential() # model is a linear stack of layers (don't change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Activation Level hyperbolic tan\n",
      "Model after first Convulution  (None, 174, 174, 8)\n",
      "Model after first MaxPooling  (None, 87, 87, 8)\n",
      "Model flattened out to  (None, 60552)\n",
      "First Dense Layer 32  (None, 32)\n",
      "2nd Dense Layer, relu  (None, 64)\n",
      "2nd Dense Layer - number of classes, softmax  (None, 6)\n"
     ]
    }
   ],
   "source": [
    "model.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]),\n",
    "                    padding='valid',\n",
    "                    input_shape=input_shape)) #first conv. layer (keep layer)\n",
    "model.add(Activation('tanh')) # Activation specification necessary for Conv2D and Dense layers\n",
    "print('Original Activation Level hyperbolic tan', )\n",
    "model.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]))) #2nd conv. layer (keep layer)\n",
    "model.add(Activation('tanh'))\n",
    "print('Model after first Convulution ', model.output_shape)\n",
    "\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=pool_size)) # decreases size, helps prevent overfitting\n",
    "model.add(Dropout(0.5)) # zeros out some fraction of inputs, helps prevent overfitting\n",
    "print('Model after first MaxPooling ', model.output_shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten()) # necessary to flatten before going into conventional dense layer (keep layer)\n",
    "print('Model flattened out to ', model.output_shape)\n",
    "\n",
    "# now start a typical neural network\n",
    "model.add(Dense(32)) # (only) 32 neurons in this layer, really?  (keep layer)\n",
    "model.add(Activation('relu'))\n",
    "print('First Dense Layer 32 ', model.output_shape)\n",
    "\n",
    "model.add(Dense(64)) # (only) 32 neurons in this layer, really?  (keep layer)\n",
    "model.add(Activation('relu'))\n",
    "print('2nd Dense Layer, relu ', model.output_shape)\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dropout(0.5)) # zeros out some fraction of inputs, helps prevent overfitting\n",
    "\n",
    "model.add(Dense(nb_classes)) # 10 final nodes (one for each class) (keep layer)\n",
    "model.add(Activation('softmax')) # keep softmax at end to pick between classes 0-9\n",
    "print('2nd Dense Layer - number of classes, softmax ', model.output_shape)\n",
    "\n",
    "# many optimizers available\n",
    "# see https://keras.io/optimizers/#usage-of-optimizers\n",
    "# suggest you keep loss at 'categorical_crossentropy' for this multiclass problem,\n",
    "# and metrics at 'accuracy'\n",
    "# suggest limiting optimizers to one of these: 'adam', 'adadelta', 'sgd'\n",
    "# how are we going to solve and evaluate it:\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78502 samples, validate on 19626 samples\n",
      "Epoch 1/4\n",
      "78502/78502 [==============================] - 3097s 39ms/step - loss: 1.0606 - acc: 0.6141 - val_loss: 0.9541 - val_acc: 0.6500\n",
      "Epoch 2/4\n",
      "78502/78502 [==============================] - 3097s 39ms/step - loss: 0.6745 - acc: 0.7575 - val_loss: 0.5395 - val_acc: 0.7902\n",
      "Epoch 3/4\n",
      "78502/78502 [==============================] - 3090s 39ms/step - loss: 0.5211 - acc: 0.8092 - val_loss: 0.4287 - val_acc: 0.8339\n",
      "Epoch 4/4\n",
      "78502/78502 [==============================] - 3103s 40ms/step - loss: 0.4076 - acc: 0.8539 - val_loss: 0.7508 - val_acc: 0.7458\n"
     ]
    }
   ],
   "source": [
    "# during fit process watch train and test error simultaneously\n",
    "cnn_fit = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, Y_test))\n",
    "cnn_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.75083406038\n",
      "Test accuracy: 0.745847345346\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1]) # this is the one we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(filepath = 'cnn_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x181c3aa860>,\n",
       " <keras.layers.core.Activation at 0x181c566d30>,\n",
       " <keras.layers.convolutional.Conv2D at 0x181c566a90>,\n",
       " <keras.layers.core.Activation at 0x181c566dd8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x181c586908>,\n",
       " <keras.layers.core.Dropout at 0x181c596a58>,\n",
       " <keras.layers.core.Flatten at 0x181c5767f0>,\n",
       " <keras.layers.core.Dense at 0x181c5dcd30>,\n",
       " <keras.layers.core.Activation at 0x181c5b8cc0>,\n",
       " <keras.layers.core.Dense at 0x181c5f1550>,\n",
       " <keras.layers.core.Activation at 0x181c625898>,\n",
       " <keras.layers.core.Dropout at 0x181c6147f0>,\n",
       " <keras.layers.core.Dense at 0x181c659630>,\n",
       " <keras.layers.core.Activation at 0x181c648320>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_pred_1 = cnn_pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.82      0.94      0.87      6706\n",
      "        1.0       0.51      0.52      0.52      1230\n",
      "        2.0       0.00      0.00      0.00       148\n",
      "        3.0       0.54      0.87      0.66      2721\n",
      "        4.0       0.99      0.63      0.77      6621\n",
      "        5.0       0.57      0.53      0.55      2200\n",
      "\n",
      "avg / total       0.78      0.75      0.74     19626\n",
      "\n",
      "confusion matrix\n",
      "\n",
      "[[6271    0   17  264   42  112]\n",
      " [   0  645    0  498    1   86]\n",
      " [   5   20    0   17    5  101]\n",
      " [ 182   38    0 2359    2  140]\n",
      " [1195  431    0  351 4203  441]\n",
      " [  16  124    2  898    0 1160]]\n",
      "0 -- white_oak\n",
      "1 -- maple\n",
      "2 -- red_oak\n",
      "3 -- walnut\n",
      "4 -- beech\n",
      "5 -- cherry\n",
      "\n",
      "\n",
      "Test score: 0.75083406038\n",
      "Test accuracy: 0.745847345346\n"
     ]
    }
   ],
   "source": [
    "print('classification report \\n')\n",
    "print(cr(y_test, cnn_pred_1))\n",
    "print('confusion matrix\\n')\n",
    "print(cm(y_test, cnn_pred_1))\n",
    "for i in range(len(wood_index_map)):\n",
    "    print(i,'--', wood_index_map[i])\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white_oak\n"
     ]
    }
   ],
   "source": [
    "print(wood_index_map[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26974.,   4946.,    548.,  10895.,  26179.,   8960.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19626, 6)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Activation Level hyperbolic tan\n",
      "Model after first Convulution  (None, 174, 174, 8)\n",
      "Model after first MaxPooling  (None, 87, 87, 8)\n",
      "Model flattened out to  (None, 60552)\n",
      "First Dense Layer 32  (None, 32)\n",
      "2nd Dense Layer, relu  (None, 64)\n",
      "3nd Dense Layer, relu  (None, 128)\n",
      "Last Dense Layer - number of classes, softmax  (None, 6)\n"
     ]
    }
   ],
   "source": [
    "model_4 = Sequential()\n",
    "model_4.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]),\n",
    "                    padding='valid',\n",
    "                    input_shape=input_shape)) #first conv. layer (keep layer)\n",
    "model_4.add(Activation('tanh')) # Activation specification necessary for Conv2D and Dense layers\n",
    "print('Original Activation Level hyperbolic tan', )\n",
    "model_4.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]))) #2nd conv. layer (keep layer)\n",
    "model_4.add(Activation('tanh'))\n",
    "print('Model after first Convulution ', model_4.output_shape)\n",
    "\n",
    "\n",
    "model_4.add(MaxPooling2D(pool_size=pool_size)) # decreases size, helps prevent overfitting\n",
    "model_4.add(Dropout(0.5)) # zeros out some fraction of inputs, helps prevent overfitting\n",
    "print('Model after first MaxPooling ', model_4.output_shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_4.add(Flatten()) # necessary to flatten before going into conventional dense layer (keep layer)\n",
    "print('Model flattened out to ', model_4.output_shape)\n",
    "\n",
    "# now start a typical neural network\n",
    "model_4.add(Dense(32)) # (only) 32 neurons in this layer, really?  (keep layer)\n",
    "model_4.add(Activation('relu'))\n",
    "print('First Dense Layer 32 ', model_4.output_shape)\n",
    "\n",
    "model_4.add(Dense(64)) # (only) 32 neurons in this layer, really?  (keep layer)\n",
    "model_4.add(Activation('relu'))\n",
    "print('2nd Dense Layer, relu ', model_4.output_shape)\n",
    "\n",
    "model_4.add(Dense(128)) # (only) 32 neurons in this layer, really?  (keep layer)\n",
    "model_4.add(Activation('relu'))\n",
    "print('3nd Dense Layer, relu ', model_4.output_shape)\n",
    "\n",
    "\n",
    "model_4.add(Dropout(0.5)) # zeros out some fraction of inputs, helps prevent overfitting\n",
    "\n",
    "model_4.add(Dense(nb_classes)) # 10 final nodes (one for each class) (keep layer)\n",
    "model_4.add(Activation('softmax')) # keep softmax at end to pick between classes 0-9\n",
    "print('Last Dense Layer - number of classes, softmax ', model_4.output_shape)\n",
    "\n",
    "model_4.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78502 samples, validate on 19626 samples\n",
      "Epoch 1/4\n",
      "78502/78502 [==============================] - 3099s 39ms/step - loss: 0.9744 - acc: 0.6429 - val_loss: 0.7066 - val_acc: 0.7444\n",
      "Epoch 2/4\n",
      "78502/78502 [==============================] - 3016s 38ms/step - loss: 0.5851 - acc: 0.7807 - val_loss: 0.4713 - val_acc: 0.8236\n",
      "Epoch 3/4\n",
      "78502/78502 [==============================] - 3070s 39ms/step - loss: 0.4319 - acc: 0.8339 - val_loss: 0.4149 - val_acc: 0.8439\n",
      "Epoch 4/4\n",
      "78502/78502 [==============================] - 3147s 40ms/step - loss: 0.3622 - acc: 0.8587 - val_loss: 0.4391 - val_acc: 0.8315\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-b2aab0997a67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m cnn_fit_4 = model_4.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\n\u001b[1;32m      3\u001b[0m           verbose=1, validation_data=(X_test, Y_test))\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcnn_pred_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# -------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscore_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_' is not defined"
     ]
    }
   ],
   "source": [
    "# during fit process watch train and test error simultaneously\n",
    "cnn_fit_4 = model_4.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, Y_test))\n",
    "cnn_pred_4 = model_.predict(X_test)\n",
    "# -------------------------------------\n",
    "score_4 = model_4.evaluate(X_test, Y_test, verbose=0)\n",
    "# -------------------------------------\n",
    "model_4.save(filepath = 'cnn_model_4')\n",
    "\n",
    "print(model_4.layers)\n",
    "\n",
    "cnn_pred_4_1 = cnn_pred_4.argmax(axis=-1)\n",
    "print('classification report \\n')\n",
    "print(cr(y_test, cnn_pred_4_1))\n",
    "print('confusion matrix\\n')\n",
    "print(cm(y_test, cnn_pred_4_1))\n",
    "for i in range(len(wood_index_map)):\n",
    "    print(i,'--', wood_index_map[i])\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('Test score:', score_4[0])\n",
    "print('Test accuracy:', score_4[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<keras.layers.convolutional.Conv2D object at 0x20a685c0f0>, <keras.layers.core.Activation object at 0x20a685c1d0>, <keras.layers.convolutional.Conv2D object at 0x20a685c4e0>, <keras.layers.core.Activation object at 0x20a685ce10>, <keras.layers.pooling.MaxPooling2D object at 0x20a72c3dd8>, <keras.layers.core.Dropout object at 0x20a6befe80>, <keras.layers.core.Flatten object at 0x20a68699e8>, <keras.layers.core.Dense object at 0x20a6c01898>, <keras.layers.core.Activation object at 0x20a6bc81d0>, <keras.layers.core.Dense object at 0x20a6bd2c50>, <keras.layers.core.Activation object at 0x20a6bb4da0>, <keras.layers.core.Dense object at 0x20a6b8a6a0>, <keras.layers.core.Activation object at 0x20a6b920b8>, <keras.layers.core.Dropout object at 0x20a6b5e278>, <keras.layers.core.Dense object at 0x20a6b8a208>, <keras.layers.core.Activation object at 0x20a6b75160>]\n",
      "classification report \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.90      0.93      6706\n",
      "        1.0       0.64      0.83      0.72      1230\n",
      "        2.0       0.00      0.00      0.00       148\n",
      "        3.0       0.72      0.57      0.64      2721\n",
      "        4.0       0.85      0.97      0.91      6621\n",
      "        5.0       0.65      0.57      0.61      2200\n",
      "\n",
      "avg / total       0.82      0.83      0.82     19626\n",
      "\n",
      "confusion matrix\n",
      "\n",
      "[[6060    2    0   63  540   41]\n",
      " [   0 1024    0    8   92  106]\n",
      " [  14    4    0   17   21   92]\n",
      " [ 234  171    0 1551  335  430]\n",
      " [  26  136    0    9 6429   21]\n",
      " [  36  265    0  494  150 1255]]\n",
      "0 -- white_oak\n",
      "1 -- maple\n",
      "2 -- red_oak\n",
      "3 -- walnut\n",
      "4 -- beech\n",
      "5 -- cherry\n",
      "\n",
      "\n",
      "Test score: 0.439119911412\n",
      "Test accuracy: 0.831499031896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dad/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "cnn_pred_4 = model_4.predict(X_test)\n",
    "# -------------------------------------\n",
    "score_4 = model_4.evaluate(X_test, Y_test, verbose=0)\n",
    "# -------------------------------------\n",
    "model_4.save(filepath = 'cnn_model_4')\n",
    "\n",
    "print(model_4.layers)\n",
    "\n",
    "cnn_pred_4_1 = cnn_pred_4.argmax(axis=-1)\n",
    "print('classification report \\n')\n",
    "print(cr(y_test, cnn_pred_4_1))\n",
    "print('confusion matrix\\n')\n",
    "print(cm(y_test, cnn_pred_4_1))\n",
    "for i in range(len(wood_index_map)):\n",
    "    print(i,'--', wood_index_map[i])\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('Test score:', score_4[0])\n",
    "print('Test accuracy:', score_4[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.90      0.93      6706\n",
      "        1.0       0.64      0.83      0.72      1230\n",
      "        2.0       0.00      0.00      0.00       148\n",
      "        3.0       0.72      0.57      0.64      2721\n",
      "        4.0       0.85      0.97      0.91      6621\n",
      "        5.0       0.65      0.57      0.61      2200\n",
      "\n",
      "avg / total       0.82      0.83      0.82     19626\n",
      "\n",
      "confusion matrix\n",
      "\n",
      "[[6060    2    0   63  540   41]\n",
      " [   0 1024    0    8   92  106]\n",
      " [  14    4    0   17   21   92]\n",
      " [ 234  171    0 1551  335  430]\n",
      " [  26  136    0    9 6429   21]\n",
      " [  36  265    0  494  150 1255]]\n",
      "0 -- white_oak\n",
      "1 -- maple\n",
      "2 -- red_oak\n",
      "3 -- walnut\n",
      "4 -- beech\n",
      "5 -- cherry\n",
      "\n",
      "\n",
      "Test accuracy: 0.831499031896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dad/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('classification report \\n')\n",
    "print(cr(y_test, cnn_pred_4_1))\n",
    "print('confusion matrix\\n')\n",
    "print(cm(y_test, cnn_pred_4_1))\n",
    "for i in range(len(wood_index_map)):\n",
    "    print(i,'--', wood_index_map[i])\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Test accuracy:', score_4[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_4.save(filepath = 'cnn_model_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Activation Level hyperbolic tan\n",
      "Model after first Convulution  (None, 166, 166, 12)\n",
      "2nd Convulution Level relu\n",
      "Model after second Convulution  (None, 159, 159, 12)\n",
      "Model after first MaxPooling  (None, 79, 79, 12)\n",
      "Model flattened out to  (None, 74892)\n",
      "First Dense Layer 32  (None, 32)\n",
      "2nd Dense Layer, relu  (None, 64)\n",
      "3nd Dense Layer, relu  (None, 128)\n",
      "Last Dense Layer - number of classes, softmax  (None, 6)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 80 # number of training samples used at a time to update the weights\n",
    "nb_epoch = 5    # number of passes\n",
    "# input image dimensions\n",
    "img_rows, img_cols = size, size \n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 12\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2) \n",
    "# decreases image size, and helps to avoid overfitting\n",
    "# convolution kernel size\n",
    "kernel_size = (8, 8) # slides over image to learn features\n",
    "# reshape image for Keras, note that image_dim_ordering set in ~.keras/keras.json\n",
    "# don't change any of this\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]),\n",
    "                    padding='valid',\n",
    "                    input_shape=input_shape)) \n",
    "#first conv. layer (keep layer)\n",
    "model.add(Activation('tanh')) # Activation specification necessary for Conv2D and Dense layers\n",
    "print('Original Activation Level hyperbolic tan', )\n",
    "model.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]))) #2nd conv. layer (keep layer)\n",
    "model.add(Activation('tanh'))\n",
    "print('Model after first Convulution ', model.output_shape)\n",
    "\n",
    "# 2nd conv layer\n",
    "model.add(Activation('relu')) # Activation specification necessary for Conv2D and Dense layers\n",
    "print('2nd Convulution Level relu', )\n",
    "model.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1])))\n",
    "model.add(Activation('relu'))\n",
    "print('Model after second Convulution ', model.output_shape)\n",
    "\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=pool_size)) # decreases size, helps prevent overfitting\n",
    "model.add(Dropout(0.2)) # zeros out some fraction of inputs, helps prevent overfitting\n",
    "print('Model after first MaxPooling ', model.output_shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten()) # necessary to flatten before going into conventional dense layer (keep layer)\n",
    "print('Model flattened out to ', model.output_shape)\n",
    "\n",
    "# now start a typical neural network\n",
    "model.add(Dense(32)) # (only) 32 neurons in this layer, really?  (keep layer)\n",
    "model.add(Activation('relu'))\n",
    "print('First Dense Layer 32 ', model.output_shape)\n",
    "\n",
    "model.add(Dense(64)) # (only) 32 neurons in this layer, really?  (keep layer)\n",
    "model.add(Activation('relu'))\n",
    "print('2nd Dense Layer, relu ', model.output_shape)\n",
    "\n",
    "model.add(Dense(128)) # (only) 32 neurons in this layer, really?  (keep layer)\n",
    "model.add(Activation('relu'))\n",
    "print('3nd Dense Layer, relu ', model.output_shape)\n",
    "\n",
    "\n",
    "model.add(Dropout(0.5)) # zeros out some fraction of inputs, helps prevent overfitting\n",
    "\n",
    "model.add(Dense(nb_classes)) # 10 final nodes (one for each class) (keep layer)\n",
    "model.add(Activation('softmax')) # keep softmax at end to pick between classes 0-9\n",
    "print('Last Dense Layer - number of classes, softmax ', model_4.output_shape)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78502 samples, validate on 19626 samples\n",
      "Epoch 1/5\n",
      "65280/78502 [=======================>......] - ETA: 1:28:43 - loss: 1.4235 - acc: 0.3600"
     ]
    }
   ],
   "source": [
    "# during fit process watch train and test error simultaneously\n",
    "cnn_fit_5 = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, Y_test))\n",
    "cnn_pred_5 = model.predict(X_test)\n",
    "# -------------------------------------\n",
    "score_5 = model_4.evaluate(X_test, Y_test, verbose=0)\n",
    "# -------------------------------------\n",
    "model.save(filepath = 'cnn_model_5')\n",
    "\n",
    "print(model.layers)\n",
    "\n",
    "cnn_pred_5_1 = cnn_pred_5.argmax(axis=-1)\n",
    "print('classification report \\n')\n",
    "print(cr(y_test, cnn_pred_5_1))\n",
    "print('confusion matrix\\n')\n",
    "print(cm(y_test, cnn_pred_5_1))\n",
    "for i in range(len(wood_index_map)):\n",
    "    print(i,'--', wood_index_map[i])\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('Test score:', score_5[0])\n",
    "print('Test accuracy:', score_5[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
